{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "GPTRick.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "77e496410b2a4817b0e15b3ef4b11c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9acceab6278a461b89d8163d9fb2005f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bc0309ed0e6b451f9aec43aca5dce2e6",
              "IPY_MODEL_718581a8fab54c388b148ffb18b14628",
              "IPY_MODEL_46a0f6713efb4ebfae66f3fc6e1dbe3b"
            ]
          }
        },
        "9acceab6278a461b89d8163d9fb2005f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc0309ed0e6b451f9aec43aca5dce2e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_edb81537268f4207b6d330b970310445",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_76622f1f97fe4e49abef78b8784e048f"
          }
        },
        "718581a8fab54c388b148ffb18b14628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_287e2cebeb6341598d98956dbb87f011",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb73a66ddd5848ca87c6d1bf86091542"
          }
        },
        "46a0f6713efb4ebfae66f3fc6e1dbe3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_17c408feb9d642c79447f39684e4ef22",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [02:48&lt;00:00, 56.23s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2dc67211744d43a4ac1a49e783706ff4"
          }
        },
        "edb81537268f4207b6d330b970310445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "76622f1f97fe4e49abef78b8784e048f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "287e2cebeb6341598d98956dbb87f011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb73a66ddd5848ca87c6d1bf86091542": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "17c408feb9d642c79447f39684e4ef22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2dc67211744d43a4ac1a49e783706ff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1470e18cc9734ed5aefc6349888eed08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_116256a93d914a6d80ed740caa9cf007",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7d8a0753c0ea426e91ae933314820379",
              "IPY_MODEL_d4a052d2756e40ec970e27e5d8cf0ce8",
              "IPY_MODEL_96223099a01f461597f2a105eccf7fe7"
            ]
          }
        },
        "116256a93d914a6d80ed740caa9cf007": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d8a0753c0ea426e91ae933314820379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fb3e340755dd41499513484d7ca6d379",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Iteration: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ac846d421e34d99b78b0e978b56f955"
          }
        },
        "d4a052d2756e40ec970e27e5d8cf0ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b2be7ec27aec48aaa5a48acf3adff077",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 93,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 93,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04c9c1de93f84a75813063ff2a297143"
          }
        },
        "96223099a01f461597f2a105eccf7fe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c8fee561c28848678d24b5f02c87db41",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 93/93 [00:56&lt;00:00,  1.38it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c728ccf9d2744edbdbc8ab1c8d10e6c"
          }
        },
        "fb3e340755dd41499513484d7ca6d379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ac846d421e34d99b78b0e978b56f955": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2be7ec27aec48aaa5a48acf3adff077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04c9c1de93f84a75813063ff2a297143": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c8fee561c28848678d24b5f02c87db41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c728ccf9d2744edbdbc8ab1c8d10e6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a6bc1e82b4e246b1ab7db20052b111cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eca2f05c523742a085578fbeb63f847b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_389201a2358445e1a415cef559e1b452",
              "IPY_MODEL_d3dff9b5c8b14e75ad1e68fc7e8450de",
              "IPY_MODEL_1f175ed0c5f8456e970332d063c6f82c"
            ]
          }
        },
        "eca2f05c523742a085578fbeb63f847b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "389201a2358445e1a415cef559e1b452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_336a8eca3d6b4ef3b988ae7b5edfcd1c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Iteration: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a79fe0ad46fd4be38abfb846f3bb65cc"
          }
        },
        "d3dff9b5c8b14e75ad1e68fc7e8450de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9ff3b3c6a9934b3fbb8fb3beba3b0370",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 93,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 93,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_62c998cd72194c5791e5953cbc990767"
          }
        },
        "1f175ed0c5f8456e970332d063c6f82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cb545843701d49a6bf8f2f53affec57d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 93/93 [00:56&lt;00:00,  1.55it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_af56c33df9a24a82a2c8e85b7f14ff01"
          }
        },
        "336a8eca3d6b4ef3b988ae7b5edfcd1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a79fe0ad46fd4be38abfb846f3bb65cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ff3b3c6a9934b3fbb8fb3beba3b0370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "62c998cd72194c5791e5953cbc990767": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb545843701d49a6bf8f2f53affec57d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "af56c33df9a24a82a2c8e85b7f14ff01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbc6cc6fa5ae4d8d9f3fd6cbcd83cba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9f7a04cfe8ef43298867dcdbc6014c3f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c7b4f2fb16df4be397c7e1d7f34b3221",
              "IPY_MODEL_369ee53967144c459e53620b4e7ee3d2",
              "IPY_MODEL_a9d15684cbe1466297ee8963167ca63a"
            ]
          }
        },
        "9f7a04cfe8ef43298867dcdbc6014c3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7b4f2fb16df4be397c7e1d7f34b3221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fc2cb814395447978c72474272c8c11d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Iteration: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13832992cac54d1ca55daee7f5c322ae"
          }
        },
        "369ee53967144c459e53620b4e7ee3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a9dcdb048b5a468f8ee69065f3f057c2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 93,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 93,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cda41d0c28b94f8bb6295df5e4cc6c15"
          }
        },
        "a9d15684cbe1466297ee8963167ca63a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4f29bcc97800469083e8136f3360817f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 93/93 [00:56&lt;00:00,  1.62it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e38df7d8012495d8346fb4e3f62e9dd"
          }
        },
        "fc2cb814395447978c72474272c8c11d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13832992cac54d1ca55daee7f5c322ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a9dcdb048b5a468f8ee69065f3f057c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cda41d0c28b94f8bb6295df5e4cc6c15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f29bcc97800469083e8136f3360817f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e38df7d8012495d8346fb4e3f62e9dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f1fb5f2395d4c21a0722a859899eacc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2e896faa24e04f7dbbcdffaa84d048a1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e137d58bb0764961a44dd635109a0bf5",
              "IPY_MODEL_62f8b436b55b4e9db821b462079b1c6e",
              "IPY_MODEL_c20d3aa286ef4ff9a84658d68178b062"
            ]
          }
        },
        "2e896faa24e04f7dbbcdffaa84d048a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e137d58bb0764961a44dd635109a0bf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9cfb4e748af54325b170da4c72200e1b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Evaluating: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f4afa913e8a14db2a3b3eadb7b7ce149"
          }
        },
        "62f8b436b55b4e9db821b462079b1c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7ec34bd52c5b47198b4474c48f531d36",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb4c5fdb020d4ce8ae9cb7f5a8223a9b"
          }
        },
        "c20d3aa286ef4ff9a84658d68178b062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_54211a0cd0a741a794adf0f433b28c07",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10/10 [00:02&lt;00:00,  4.68it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b2fe5cbbc42647fbbf0e488d06f96d48"
          }
        },
        "9cfb4e748af54325b170da4c72200e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f4afa913e8a14db2a3b3eadb7b7ce149": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ec34bd52c5b47198b4474c48f531d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb4c5fdb020d4ce8ae9cb7f5a8223a9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "54211a0cd0a741a794adf0f433b28c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b2fe5cbbc42647fbbf0e488d06f96d48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Make your own Rick Sanchez (bot) with Transformers and DialoGPT fine-tuning\n",
        "\n",
        "by [Rostyslav Neskorozhenyi](https://www.linkedin.com/in/slanj)"
      ],
      "metadata": {
        "id": "7KrNfVNueNhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A bit of theory"
      ],
      "metadata": {
        "id": "yGJHOZYOWZHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this article I will tell you how to create a virtual character whose statements will be based on a transcript of my favorite animated science fiction sitcom. You can use characters and sitcoms of your choice. I added code to convert a regular text file with dialogs into a format that the model understands.\n",
        "\n",
        "As I already mentioned, [Transformers](https://huggingface.co/transformers/) library, which contains the latest NLP models (such as [BERT](https://huggingface.co/transformers/model_doc/bert.html), [XLNet](https://huggingface.co/transformers/model_doc/xlnet.html), [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)) will help us in our task. You can read more about transformers in a beautifully illustrated [article](http://jalammar.github.io/illustrated-transformer/) by Jay Alammar.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png) Image from[ http://jalammar.github.io](http://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "Not so long ago, Microsoft’s [DialoGPT](https://huggingface.co/transformers/model_doc/dialogpt.html) was added to the Transformers model collection. DialoGPT is a GPT-2 model, trained on 147M multi-turn dialogue from Reddit discussion thread (you can learn more about GPT-2 [here](http://jalammar.github.io/illustrated-gpt2/)). This model is ideally suited for creating a virtual character for a fascinating conversation and even in the small implementation option it can maintain a coherent dialogue, which we will see now.\n",
        "\n"
      ],
      "metadata": {
        "id": "YBd8PVGNWgfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First dialogue with DialoGPT"
      ],
      "metadata": {
        "id": "4hyTyW7Le2Sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will conduct all our experiments in Google Colab, its resources are enough to train the small DialoGPT model. Firstly, we will connect to Google Drive and install the necessary modules."
      ],
      "metadata": {
        "id": "HEqpXb4ofFGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "metadata": {
        "id": "sgL1yLLS09mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb435e0a-d14e-4a49-efc8-173c6721b5f1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "! pip -q install transformers"
      ],
      "outputs": [],
      "metadata": {
        "id": "onU41i8g1J3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's move to the desired folder in which we will store all our data."
      ],
      "metadata": {
        "id": "d7Y1XcsqgXBQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "import os\r\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "OHNacD0k1HI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to chat with DialoGPT without fine-tuning."
      ],
      "metadata": {
        "id": "zVSTlysV2jaM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\r\n",
        "import torch\r\n",
        "\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\r\n",
        "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:664: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "metadata": {
        "id": "w6qrl7_SvPKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ac8fd8-0cfc-4c29-96fb-da791836de67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model initial configuration"
      ],
      "metadata": {
        "id": "IuzSROqxjUKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train our own Rick chatbot. For start, we will need basic configuration and a dataset.\n",
        "Configuration and training scripts are mostly based on this [script](https://github.com/huggingface/transformers/blob/master/examples/legacy/run_language_modeling.py) from Huggingface and great [tutorial](https://nathancooper.io/i-am-a-nerd/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html) from Nathan Cooper.\n",
        "\n",
        "__*Note from the future:*__ The linked script from huggingface is deprecated. The correct path for the scripts is [here](https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling). See also [this error](https://stackoverflow.com/questions/67089849/attributeerror-gpt2tokenizerfast-object-has-no-attribute-max-len)."
      ],
      "metadata": {
        "id": "TC3qNlfp30aU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "\"\"\"\r\n",
        "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\r\n",
        "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\r\n",
        "using a masked language modeling (MLM) loss.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "import glob\r\n",
        "import logging\r\n",
        "import os\r\n",
        "import pickle\r\n",
        "import random\r\n",
        "import re\r\n",
        "import shutil\r\n",
        "from typing import Dict, List, Tuple\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "from torch.nn.utils.rnn import pad_sequence\r\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\r\n",
        "from torch.utils.data.distributed import DistributedSampler\r\n",
        "from tqdm.notebook import tqdm, trange\r\n",
        "\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "from transformers import (\r\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\r\n",
        "    WEIGHTS_NAME,\r\n",
        "    AdamW,\r\n",
        "    AutoConfig,\r\n",
        "    AutoModelWithLMHead,\r\n",
        "    AutoTokenizer,\r\n",
        "    PreTrainedModel,\r\n",
        "    PreTrainedTokenizer,\r\n",
        "    get_linear_schedule_with_warmup,\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "try:\r\n",
        "    from torch.utils.tensorboard import SummaryWriter\r\n",
        "except ImportError:\r\n",
        "    from tensorboardX import SummaryWriter\r\n",
        "\r\n",
        "# Configs\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "\r\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\r\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
      ],
      "outputs": [],
      "metadata": {
        "id": "g91QzdqU2haO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "# Args to allow for easy convertion of python script to notebook\r\n",
        "class Args():\r\n",
        "    def __init__(self):\r\n",
        "        self.output_dir = 'output-small-rick' #changed output directory\r\n",
        "        self.model_type = 'gpt2'\r\n",
        "        self.model_name_or_path = 'microsoft/DialoGPT-small'\r\n",
        "        self.config_name = 'microsoft/DialoGPT-small'\r\n",
        "        self.tokenizer_name = 'microsoft/DialoGPT-small'\r\n",
        "        self.cache_dir = 'cached'\r\n",
        "        self.block_size = 512\r\n",
        "        self.do_train = True\r\n",
        "        self.do_eval = True\r\n",
        "        self.evaluate_during_training = False\r\n",
        "        self.per_gpu_train_batch_size = 4\r\n",
        "        self.per_gpu_eval_batch_size = 4\r\n",
        "        self.gradient_accumulation_steps = 1\r\n",
        "        self.learning_rate = 5e-5\r\n",
        "        self.weight_decay = 0.0\r\n",
        "        self.adam_epsilon = 1e-8\r\n",
        "        self.max_grad_norm = 1.0\r\n",
        "        self.num_train_epochs = 3\r\n",
        "        self.max_steps = -1\r\n",
        "        self.warmup_steps = 0\r\n",
        "        self.logging_steps = 1000\r\n",
        "        self.save_steps = 3500\r\n",
        "        self.save_total_limit = None\r\n",
        "        self.eval_all_checkpoints = False\r\n",
        "        self.no_cuda = False\r\n",
        "        self.overwrite_output_dir = True\r\n",
        "        self.overwrite_cache = True\r\n",
        "        self.should_continue = False\r\n",
        "        self.seed = 42\r\n",
        "        self.local_rank = -1\r\n",
        "        self.fp16 = False\r\n",
        "        self.fp16_opt_level = 'O1'\r\n",
        "\r\n",
        "args = Args()"
      ],
      "outputs": [],
      "metadata": {
        "id": "utprDGf06OVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset"
      ],
      "metadata": {
        "id": "X_qYqlTe9yx2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "!git clone https://github.com/gullabi/debatebot.git"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'debatebot'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 71 (delta 11), reused 64 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (71/71), done.\n"
          ]
        }
      ],
      "metadata": {
        "id": "zbB-56mmm-rL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb76563-ee5f-4619-f98a-f6c634b5acb4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "# Let's look at original dataset\r\n",
        "all_rick = pd.read_csv('debatebot/data/tutorial/RickAndMortyScripts.csv')\r\n",
        "all_rick.head(10)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>season no.</th>\n",
              "      <th>episode no.</th>\n",
              "      <th>episode name</th>\n",
              "      <th>name</th>\n",
              "      <th>line</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Rick</td>\n",
              "      <td>Morty! You gotta come on. Jus'... you gotta co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Morty</td>\n",
              "      <td>What, Rick? What’s going on?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Rick</td>\n",
              "      <td>I got a surprise for you, Morty.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Morty</td>\n",
              "      <td>It's the middle of the night. What are you tal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Rick</td>\n",
              "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Morty</td>\n",
              "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Rick</td>\n",
              "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Rick</td>\n",
              "      <td>What do you think of this... flying vehicle, M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Morty</td>\n",
              "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Rick</td>\n",
              "      <td>Morty. I had to... I had to do it. I had— I ha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  season no.  ...   name                                               line\n",
              "0      0           1  ...   Rick  Morty! You gotta come on. Jus'... you gotta co...\n",
              "1      1           1  ...  Morty                       What, Rick? What’s going on?\n",
              "2      2           1  ...   Rick                   I got a surprise for you, Morty.\n",
              "3      3           1  ...  Morty  It's the middle of the night. What are you tal...\n",
              "4      4           1  ...   Rick  Come on, I got a surprise for you.  Come on, h...\n",
              "5      5           1  ...  Morty                Ow! Ow! You're tugging me too hard!\n",
              "6      6           1  ...   Rick  We gotta go, gotta get outta here, come on. Go...\n",
              "7      7           1  ...   Rick  What do you think of this... flying vehicle, M...\n",
              "8      8           1  ...  Morty  Yeah, Rick... I-it's great. Is this the surprise?\n",
              "9      9           1  ...   Rick  Morty. I had to... I had to do it. I had— I ha...\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "metadata": {
        "id": "TxHdyv67dptm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "f51f66f1-88b0-4432-f6cc-49415923a0a4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will convert this dataset in a way that every responce row will contain **n** previous responces as a context. For our purposes seven previous responces will be enough."
      ],
      "metadata": {
        "id": "n51hsl2mAG8v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "contexted = []\r\n",
        "\r\n",
        "n = 7\r\n",
        "\r\n",
        "for i in range(n, len(all_rick['line'])):\r\n",
        "  # Only select Rick's responses\r\n",
        "  if all_rick['name'][i] == \"Rick\":\r\n",
        "    row = []\r\n",
        "    prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \r\n",
        "    for j in range(i, prev, -1):\r\n",
        "      row.append(all_rick['line'][j])\r\n",
        "    contexted.append(row)  "
      ],
      "outputs": [],
      "metadata": {
        "id": "WyL7Dx1bbLRC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "len(contexted)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "416"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "metadata": {
        "id": "R_cwbBolVi14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ea8802-f96e-4c5b-c459-e7fcc266c5d2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "columns = ['response', 'context'] \r\n",
        "columns = columns + ['context/'+str(i) for i in range(n-1)]\r\n",
        "columns"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['response',\n",
              " 'context',\n",
              " 'context/0',\n",
              " 'context/1',\n",
              " 'context/2',\n",
              " 'context/3',\n",
              " 'context/4',\n",
              " 'context/5']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "metadata": {
        "id": "gNkYLty-VhTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba5ddb5-45b7-44e2-e2b4-0a3c4f65e4a0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "df = pd.DataFrame.from_records(contexted, columns=columns)\r\n",
        "df.head(5)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "      <th>context</th>\n",
              "      <th>context/0</th>\n",
              "      <th>context/1</th>\n",
              "      <th>context/2</th>\n",
              "      <th>context/3</th>\n",
              "      <th>context/4</th>\n",
              "      <th>context/5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What do you think of this... flying vehicle, M...</td>\n",
              "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
              "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
              "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
              "      <td>It's the middle of the night. What are you tal...</td>\n",
              "      <td>I got a surprise for you, Morty.</td>\n",
              "      <td>What, Rick? What’s going on?</td>\n",
              "      <td>Morty! You gotta come on. Jus'... you gotta co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Morty. I had to... I had to do it. I had— I ha...</td>\n",
              "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
              "      <td>What do you think of this... flying vehicle, M...</td>\n",
              "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
              "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
              "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
              "      <td>It's the middle of the night. What are you tal...</td>\n",
              "      <td>I got a surprise for you, Morty.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We're gonna drop it down there just get a whol...</td>\n",
              "      <td>What?! A bomb?!</td>\n",
              "      <td>Morty. I had to... I had to do it. I had— I ha...</td>\n",
              "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
              "      <td>What do you think of this... flying vehicle, M...</td>\n",
              "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
              "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
              "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Come on, Morty. Just take it easy, Morty. It's...</td>\n",
              "      <td>T-t-that's absolutely crazy!</td>\n",
              "      <td>We're gonna drop it down there just get a whol...</td>\n",
              "      <td>What?! A bomb?!</td>\n",
              "      <td>Morty. I had to... I had to do it. I had— I ha...</td>\n",
              "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
              "      <td>What do you think of this... flying vehicle, M...</td>\n",
              "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>When I drop the bomb you know, I want you to h...</td>\n",
              "      <td>Jessica? From my math class?</td>\n",
              "      <td>Come on, Morty. Just take it easy, Morty. It's...</td>\n",
              "      <td>T-t-that's absolutely crazy!</td>\n",
              "      <td>We're gonna drop it down there just get a whol...</td>\n",
              "      <td>What?! A bomb?!</td>\n",
              "      <td>Morty. I had to... I had to do it. I had— I ha...</td>\n",
              "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            response  ...                                          context/5\n",
              "0  What do you think of this... flying vehicle, M...  ...  Morty! You gotta come on. Jus'... you gotta co...\n",
              "1  Morty. I had to... I had to do it. I had— I ha...  ...                   I got a surprise for you, Morty.\n",
              "2  We're gonna drop it down there just get a whol...  ...  Come on, I got a surprise for you.  Come on, h...\n",
              "3  Come on, Morty. Just take it easy, Morty. It's...  ...  We gotta go, gotta get outta here, come on. Go...\n",
              "4  When I drop the bomb you know, I want you to h...  ...  Yeah, Rick... I-it's great. Is this the surprise?\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "metadata": {
        "id": "kPafxqIYgurW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "10cb113e-ea3e-46f0-ffc6-ac9a17fd0b77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split our dataset into a training and test parts."
      ],
      "metadata": {
        "id": "aBeM8pvEjigq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "trn_df, val_df = train_test_split(df, test_size = 0.1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "g1CeutVVlL85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now will convert our dataset in a format suitable for our model. Basically we will concatenate responses in one string for each row (additionally we will add special 'end of string' token between responses, so the model will understand end of each response in a string).  "
      ],
      "metadata": {
        "id": "86F3WhnFO4H8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "source": [
        "def construct_conv(row, tokenizer, eos = True):\r\n",
        "    flatten = lambda l: [item for sublist in l for item in sublist]\r\n",
        "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\r\n",
        "    conv = flatten(conv)\r\n",
        "    return conv\r\n",
        "\r\n",
        "class ConversationDataset(Dataset):\r\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\r\n",
        "\r\n",
        "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\r\n",
        "\r\n",
        "        directory = args.cache_dir\r\n",
        "        cached_features_file = os.path.join(\r\n",
        "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\r\n",
        "        )\r\n",
        "\r\n",
        "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\r\n",
        "            logger.info(\"Loading features from cached file %s\", cached_features_file)\r\n",
        "            with open(cached_features_file, \"rb\") as handle:\r\n",
        "                self.examples = pickle.load(handle)\r\n",
        "        else:\r\n",
        "            logger.info(\"Creating features from dataset file at %s\", directory)\r\n",
        "\r\n",
        "            self.examples = []\r\n",
        "            for _, row in df.iterrows():\r\n",
        "                conv = construct_conv(row, tokenizer)\r\n",
        "                self.examples.append(conv)\r\n",
        "\r\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\r\n",
        "            with open(cached_features_file, \"wb\") as handle:\r\n",
        "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.examples)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        return torch.tensor(self.examples[item], dtype=torch.long)"
      ],
      "outputs": [],
      "metadata": {
        "id": "PX7jeWpYmOe_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "source": [
        "# Cacheing and storing of data/checkpoints\r\n",
        "\r\n",
        "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\r\n",
        "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\r\n",
        "\r\n",
        "\r\n",
        "def set_seed(args):\r\n",
        "    random.seed(args.seed)\r\n",
        "    np.random.seed(args.seed)\r\n",
        "    torch.manual_seed(args.seed)\r\n",
        "    if args.n_gpu > 0:\r\n",
        "        torch.cuda.manual_seed_all(args.seed)\r\n",
        "\r\n",
        "\r\n",
        "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\r\n",
        "    ordering_and_checkpoint_path = []\r\n",
        "\r\n",
        "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\r\n",
        "\r\n",
        "    for path in glob_checkpoints:\r\n",
        "        if use_mtime:\r\n",
        "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\r\n",
        "        else:\r\n",
        "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\r\n",
        "            if regex_match and regex_match.groups():\r\n",
        "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\r\n",
        "\r\n",
        "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\r\n",
        "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\r\n",
        "    return checkpoints_sorted\r\n",
        "\r\n",
        "\r\n",
        "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\r\n",
        "    if not args.save_total_limit:\r\n",
        "        return\r\n",
        "    if args.save_total_limit <= 0:\r\n",
        "        return\r\n",
        "\r\n",
        "    # Check if we should delete older checkpoint(s)\r\n",
        "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\r\n",
        "    if len(checkpoints_sorted) <= args.save_total_limit:\r\n",
        "        return\r\n",
        "\r\n",
        "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\r\n",
        "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\r\n",
        "    for checkpoint in checkpoints_to_be_deleted:\r\n",
        "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\r\n",
        "        shutil.rmtree(checkpoint)"
      ],
      "outputs": [],
      "metadata": {
        "id": "naaRHoXgnStq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluating\r\n",
        "\r\n",
        "There will be quite a lot of code needed for training our model but don’t worry, everything should work as is, the main thing is to give the model the dataset in the right format."
      ],
      "metadata": {
        "id": "pkvMNnrnVHQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "source": [
        "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\r\n",
        "    \"\"\" Train the model \"\"\"\r\n",
        "    if args.local_rank in [-1, 0]:\r\n",
        "        tb_writer = SummaryWriter()\r\n",
        "\r\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\r\n",
        "\r\n",
        "    def collate(examples: List[torch.Tensor]):\r\n",
        "        if tokenizer._pad_token is None:\r\n",
        "            return pad_sequence(examples, batch_first=True)\r\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\r\n",
        "\r\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\r\n",
        "    train_dataloader = DataLoader(\r\n",
        "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\r\n",
        "    )\r\n",
        "\r\n",
        "    if args.max_steps > 0:\r\n",
        "        t_total = args.max_steps\r\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\r\n",
        "    else:\r\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\r\n",
        "\r\n",
        "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\r\n",
        "    model.resize_token_embeddings(len(tokenizer))\r\n",
        "    # add_special_tokens_(model, tokenizer)\r\n",
        "\r\n",
        "\r\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\r\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\r\n",
        "    optimizer_grouped_parameters = [\r\n",
        "        {\r\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\r\n",
        "            \"weight_decay\": args.weight_decay,\r\n",
        "        },\r\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\r\n",
        "    ]\r\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\r\n",
        "    scheduler = get_linear_schedule_with_warmup(\r\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\r\n",
        "    )\r\n",
        "\r\n",
        "    # Check if saved optimizer or scheduler states exist\r\n",
        "    if (\r\n",
        "        args.model_name_or_path\r\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\r\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\r\n",
        "    ):\r\n",
        "        # Load in optimizer and scheduler states\r\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\r\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\r\n",
        "\r\n",
        "    if args.fp16:\r\n",
        "        try:\r\n",
        "            from apex import amp\r\n",
        "        except ImportError:\r\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\r\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\r\n",
        "\r\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\r\n",
        "    if args.n_gpu > 1:\r\n",
        "        model = torch.nn.DataParallel(model)\r\n",
        "\r\n",
        "    # Distributed training (should be after apex fp16 initialization)\r\n",
        "    if args.local_rank != -1:\r\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\r\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\r\n",
        "        )\r\n",
        "\r\n",
        "    # Train!\r\n",
        "    logger.info(\"***** Running training *****\")\r\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\r\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\r\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\r\n",
        "    logger.info(\r\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\r\n",
        "        args.train_batch_size\r\n",
        "        * args.gradient_accumulation_steps\r\n",
        "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\r\n",
        "    )\r\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\r\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\r\n",
        "\r\n",
        "    global_step = 0\r\n",
        "    epochs_trained = 0\r\n",
        "    steps_trained_in_current_epoch = 0\r\n",
        "    # Check if continuing training from a checkpoint\r\n",
        "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\r\n",
        "        try:\r\n",
        "            # set global_step to gobal_step of last saved checkpoint from model path\r\n",
        "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\r\n",
        "            global_step = int(checkpoint_suffix)\r\n",
        "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\r\n",
        "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\r\n",
        "\r\n",
        "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\r\n",
        "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\r\n",
        "            logger.info(\"  Continuing training from global step %d\", global_step)\r\n",
        "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\r\n",
        "        except ValueError:\r\n",
        "            logger.info(\"  Starting fine-tuning.\")\r\n",
        "\r\n",
        "    tr_loss, logging_loss = 0.0, 0.0\r\n",
        "\r\n",
        "    model.zero_grad()\r\n",
        "    train_iterator = trange(\r\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\r\n",
        "    )\r\n",
        "    set_seed(args)  # Added here for reproducibility\r\n",
        "    for _ in train_iterator:\r\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\r\n",
        "        for step, batch in enumerate(epoch_iterator):\r\n",
        "\r\n",
        "            # Skip past any already trained steps if resuming training\r\n",
        "            if steps_trained_in_current_epoch > 0:\r\n",
        "                steps_trained_in_current_epoch -= 1\r\n",
        "                continue\r\n",
        "\r\n",
        "            inputs, labels = (batch, batch)\r\n",
        "            if inputs.shape[1] > 1024: continue\r\n",
        "            inputs = inputs.to(args.device)\r\n",
        "            labels = labels.to(args.device)\r\n",
        "            model.train()\r\n",
        "            outputs = model(inputs, labels=labels)\r\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\r\n",
        "\r\n",
        "            if args.n_gpu > 1:\r\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\r\n",
        "            if args.gradient_accumulation_steps > 1:\r\n",
        "                loss = loss / args.gradient_accumulation_steps\r\n",
        "\r\n",
        "            if args.fp16:\r\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\r\n",
        "                    scaled_loss.backward()\r\n",
        "            else:\r\n",
        "                loss.backward()\r\n",
        "\r\n",
        "            tr_loss += loss.item()\r\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\r\n",
        "                if args.fp16:\r\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\r\n",
        "                else:\r\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\r\n",
        "                optimizer.step()\r\n",
        "                scheduler.step()  # Update learning rate schedule\r\n",
        "                model.zero_grad()\r\n",
        "                global_step += 1\r\n",
        "\r\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\r\n",
        "                    # Log metrics\r\n",
        "                    if (\r\n",
        "                        args.local_rank == -1 and args.evaluate_during_training\r\n",
        "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\r\n",
        "                        results = evaluate(args, model, tokenizer)\r\n",
        "                        for key, value in results.items():\r\n",
        "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\r\n",
        "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\r\n",
        "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\r\n",
        "                    logging_loss = tr_loss\r\n",
        "\r\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\r\n",
        "                    checkpoint_prefix = \"checkpoint\"\r\n",
        "                    # Save model checkpoint\r\n",
        "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\r\n",
        "                    os.makedirs(output_dir, exist_ok=True)\r\n",
        "                    model_to_save = (\r\n",
        "                        model.module if hasattr(model, \"module\") else model\r\n",
        "                    )  # Take care of distributed/parallel training\r\n",
        "                    model_to_save.save_pretrained(output_dir)\r\n",
        "                    tokenizer.save_pretrained(output_dir)\r\n",
        "\r\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\r\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\r\n",
        "\r\n",
        "                    _rotate_checkpoints(args, checkpoint_prefix)\r\n",
        "\r\n",
        "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\r\n",
        "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\r\n",
        "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\r\n",
        "\r\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\r\n",
        "                epoch_iterator.close()\r\n",
        "                break\r\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\r\n",
        "            train_iterator.close()\r\n",
        "            break\r\n",
        "\r\n",
        "    if args.local_rank in [-1, 0]:\r\n",
        "        tb_writer.close()\r\n",
        "\r\n",
        "    return global_step, tr_loss / global_step\r\n",
        "\r\n",
        "# Evaluation of some model\r\n",
        "\r\n",
        "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\r\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\r\n",
        "    eval_output_dir = args.output_dir\r\n",
        "\r\n",
        "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\r\n",
        "    os.makedirs(eval_output_dir, exist_ok=True)\r\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\r\n",
        "    # Note that DistributedSampler samples randomly\r\n",
        "\r\n",
        "    def collate(examples: List[torch.Tensor]):\r\n",
        "        if tokenizer._pad_token is None:\r\n",
        "            return pad_sequence(examples, batch_first=True)\r\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\r\n",
        "\r\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\r\n",
        "    eval_dataloader = DataLoader(\r\n",
        "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\r\n",
        "    )\r\n",
        "\r\n",
        "    # multi-gpu evaluate\r\n",
        "    if args.n_gpu > 1:\r\n",
        "        model = torch.nn.DataParallel(model)\r\n",
        "\r\n",
        "    # Eval!\r\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\r\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\r\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\r\n",
        "    eval_loss = 0.0\r\n",
        "    nb_eval_steps = 0\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\r\n",
        "        inputs, labels = (batch, batch)\r\n",
        "        inputs = inputs.to(args.device)\r\n",
        "        labels = labels.to(args.device)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            outputs = model(inputs, labels=labels)\r\n",
        "            lm_loss = outputs[0]\r\n",
        "            eval_loss += lm_loss.mean().item()\r\n",
        "        nb_eval_steps += 1\r\n",
        "\r\n",
        "    eval_loss = eval_loss / nb_eval_steps\r\n",
        "    perplexity = torch.exp(torch.tensor(eval_loss))\r\n",
        "\r\n",
        "    result = {\"perplexity\": perplexity}\r\n",
        "\r\n",
        "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\r\n",
        "    with open(output_eval_file, \"w\") as writer:\r\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\r\n",
        "        for key in sorted(result.keys()):\r\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\r\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\r\n",
        "\r\n",
        "    return result"
      ],
      "outputs": [],
      "metadata": {
        "id": "tXzKlXHeu0Mb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "source": [
        "# Main runner\r\n",
        "\r\n",
        "def main(df_trn, df_val):\r\n",
        "    args = Args()\r\n",
        "    \r\n",
        "    if args.should_continue:\r\n",
        "        sorted_checkpoints = _sorted_checkpoints(args)\r\n",
        "        if len(sorted_checkpoints) == 0:\r\n",
        "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\r\n",
        "        else:\r\n",
        "            args.model_name_or_path = sorted_checkpoints[-1]\r\n",
        "\r\n",
        "    if (\r\n",
        "        os.path.exists(args.output_dir)\r\n",
        "        and os.listdir(args.output_dir)\r\n",
        "        and args.do_train\r\n",
        "        and not args.overwrite_output_dir\r\n",
        "        and not args.should_continue\r\n",
        "    ):\r\n",
        "        raise ValueError(\r\n",
        "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\r\n",
        "                args.output_dir\r\n",
        "            )\r\n",
        "        )\r\n",
        "\r\n",
        "    # Setup CUDA, GPU & distributed training\r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    args.n_gpu = torch.cuda.device_count()\r\n",
        "    args.device = device\r\n",
        "\r\n",
        "    # Setup logging\r\n",
        "    logging.basicConfig(\r\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\r\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\r\n",
        "    )\r\n",
        "    logger.warning(\r\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\r\n",
        "        args.local_rank,\r\n",
        "        device,\r\n",
        "        args.n_gpu,\r\n",
        "        bool(args.local_rank != -1),\r\n",
        "        args.fp16,\r\n",
        "    )\r\n",
        "\r\n",
        "    # Set seed\r\n",
        "    set_seed(args)\r\n",
        "\r\n",
        "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\r\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\r\n",
        "    model = AutoModelWithLMHead.from_pretrained(\r\n",
        "        args.model_name_or_path,\r\n",
        "        from_tf=False,\r\n",
        "        config=config,\r\n",
        "        cache_dir=args.cache_dir,\r\n",
        "    )\r\n",
        "    model.to(args.device)\r\n",
        "    \r\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\r\n",
        "\r\n",
        "    # Training\r\n",
        "    if args.do_train:\r\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\r\n",
        "\r\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\r\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\r\n",
        "\r\n",
        "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\r\n",
        "    if args.do_train:\r\n",
        "        # Create output directory if needed\r\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\r\n",
        "\r\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\r\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\r\n",
        "        # They can then be reloaded using `from_pretrained()`\r\n",
        "        model_to_save = (\r\n",
        "            model.module if hasattr(model, \"module\") else model\r\n",
        "        )  # Take care of distributed/parallel training\r\n",
        "        model_to_save.save_pretrained(args.output_dir)\r\n",
        "        tokenizer.save_pretrained(args.output_dir)\r\n",
        "\r\n",
        "        # Good practice: save your training arguments together with the trained model\r\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\r\n",
        "\r\n",
        "        # Load a trained model and vocabulary that you have fine-tuned\r\n",
        "        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\r\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\r\n",
        "        model.to(args.device)\r\n",
        "\r\n",
        "    # Evaluation\r\n",
        "    results = {}\r\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\r\n",
        "        checkpoints = [args.output_dir]\r\n",
        "        if args.eval_all_checkpoints:\r\n",
        "            checkpoints = list(\r\n",
        "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\r\n",
        "            )\r\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\r\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\r\n",
        "        for checkpoint in checkpoints:\r\n",
        "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\r\n",
        "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\r\n",
        "\r\n",
        "            model = AutoModelWithLMHead.from_pretrained(checkpoint)\r\n",
        "            model.to(args.device)\r\n",
        "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\r\n",
        "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\r\n",
        "            results.update(result)\r\n",
        "\r\n",
        "    return results"
      ],
      "outputs": [],
      "metadata": {
        "id": "-MGD6bFXV4Z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is time to train our model!"
      ],
      "metadata": {
        "id": "UZEHDzR0Vjs7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "main(trn_df, val_df)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/02/2021 10:50:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:664: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "10/02/2021 10:50:14 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x7f154528b810>\n",
            "10/02/2021 10:50:14 - INFO - __main__ -   Creating features from dataset file at cached\n",
            "10/02/2021 10:50:16 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
            "10/02/2021 10:50:16 - INFO - __main__ -   ***** Running training *****\n",
            "10/02/2021 10:50:16 - INFO - __main__ -     Num examples = 374\n",
            "10/02/2021 10:50:16 - INFO - __main__ -     Num Epochs = 3\n",
            "10/02/2021 10:50:16 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "10/02/2021 10:50:16 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "10/02/2021 10:50:16 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "10/02/2021 10:50:16 - INFO - __main__ -     Total optimization steps = 279\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77e496410b2a4817b0e15b3ef4b11c65",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1470e18cc9734ed5aefc6349888eed08",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Iteration:   0%|          | 0/93 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6bc1e82b4e246b1ab7db20052b111cb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Iteration:   0%|          | 0/93 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbc6cc6fa5ae4d8d9f3fd6cbcd83cba3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Iteration:   0%|          | 0/93 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/02/2021 10:53:05 - INFO - __main__ -    global_step = 279, average loss = 2.9513507953254123\n",
            "10/02/2021 10:53:05 - INFO - __main__ -   Saving model checkpoint to output-small-rick\n",
            "10/02/2021 10:53:10 - INFO - __main__ -   Evaluate the following checkpoints: ['output-small-rick']\n",
            "10/02/2021 10:53:12 - INFO - __main__ -   Creating features from dataset file at cached\n",
            "10/02/2021 10:53:12 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
            "10/02/2021 10:53:12 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/02/2021 10:53:12 - INFO - __main__ -     Num examples = 42\n",
            "10/02/2021 10:53:12 - INFO - __main__ -     Batch size = 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f1fb5f2395d4c21a0722a859899eacc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/02/2021 10:53:15 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/02/2021 10:53:15 - INFO - __main__ -     perplexity = tensor(6.8838)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'perplexity_': tensor(6.8838)}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "metadata": {
        "id": "__iqR8YFV-Ex",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613,
          "referenced_widgets": [
            "77e496410b2a4817b0e15b3ef4b11c65",
            "9acceab6278a461b89d8163d9fb2005f",
            "bc0309ed0e6b451f9aec43aca5dce2e6",
            "718581a8fab54c388b148ffb18b14628",
            "46a0f6713efb4ebfae66f3fc6e1dbe3b",
            "edb81537268f4207b6d330b970310445",
            "76622f1f97fe4e49abef78b8784e048f",
            "287e2cebeb6341598d98956dbb87f011",
            "fb73a66ddd5848ca87c6d1bf86091542",
            "17c408feb9d642c79447f39684e4ef22",
            "2dc67211744d43a4ac1a49e783706ff4",
            "1470e18cc9734ed5aefc6349888eed08",
            "116256a93d914a6d80ed740caa9cf007",
            "7d8a0753c0ea426e91ae933314820379",
            "d4a052d2756e40ec970e27e5d8cf0ce8",
            "96223099a01f461597f2a105eccf7fe7",
            "fb3e340755dd41499513484d7ca6d379",
            "4ac846d421e34d99b78b0e978b56f955",
            "b2be7ec27aec48aaa5a48acf3adff077",
            "04c9c1de93f84a75813063ff2a297143",
            "c8fee561c28848678d24b5f02c87db41",
            "8c728ccf9d2744edbdbc8ab1c8d10e6c",
            "a6bc1e82b4e246b1ab7db20052b111cb",
            "eca2f05c523742a085578fbeb63f847b",
            "389201a2358445e1a415cef559e1b452",
            "d3dff9b5c8b14e75ad1e68fc7e8450de",
            "1f175ed0c5f8456e970332d063c6f82c",
            "336a8eca3d6b4ef3b988ae7b5edfcd1c",
            "a79fe0ad46fd4be38abfb846f3bb65cc",
            "9ff3b3c6a9934b3fbb8fb3beba3b0370",
            "62c998cd72194c5791e5953cbc990767",
            "cb545843701d49a6bf8f2f53affec57d",
            "af56c33df9a24a82a2c8e85b7f14ff01",
            "bbc6cc6fa5ae4d8d9f3fd6cbcd83cba3",
            "9f7a04cfe8ef43298867dcdbc6014c3f",
            "c7b4f2fb16df4be397c7e1d7f34b3221",
            "369ee53967144c459e53620b4e7ee3d2",
            "a9d15684cbe1466297ee8963167ca63a",
            "fc2cb814395447978c72474272c8c11d",
            "13832992cac54d1ca55daee7f5c322ae",
            "a9dcdb048b5a468f8ee69065f3f057c2",
            "cda41d0c28b94f8bb6295df5e4cc6c15",
            "4f29bcc97800469083e8136f3360817f",
            "0e38df7d8012495d8346fb4e3f62e9dd",
            "0f1fb5f2395d4c21a0722a859899eacc",
            "2e896faa24e04f7dbbcdffaa84d048a1",
            "e137d58bb0764961a44dd635109a0bf5",
            "62f8b436b55b4e9db821b462079b1c6e",
            "c20d3aa286ef4ff9a84658d68178b062",
            "9cfb4e748af54325b170da4c72200e1b",
            "f4afa913e8a14db2a3b3eadb7b7ce149",
            "7ec34bd52c5b47198b4474c48f531d36",
            "fb4c5fdb020d4ce8ae9cb7f5a8223a9b",
            "54211a0cd0a741a794adf0f433b28c07",
            "b2fe5cbbc42647fbbf0e488d06f96d48"
          ]
        },
        "outputId": "a13acbf5-c0d0-4135-d320-1d31371b9a0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatting with  Rick"
      ],
      "metadata": {
        "id": "6eDkPEuvbD47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is ready, so it's time to chat with Rick. But don't forget that Rick can be rude, I warned you.\r\n",
        "\r\n",
        "A variety of methods can be used in responces generation. You can find more details about these methods by this [link](https://huggingface.co/blog/how-to-generate). "
      ],
      "metadata": {
        "id": "rjVqotI05gOS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\r\n",
        "model = AutoModelWithLMHead.from_pretrained('output-small-rick') #change model location\r\n",
        "\r\n",
        "# Let's chat for 5 lines\r\n",
        "for step in range(5):\r\n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\r\n",
        "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\r\n",
        "    # print(new_user_input_ids)\r\n",
        "\r\n",
        "    # append the new user input tokens to the chat history\r\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\r\n",
        "\r\n",
        "    # generated a response while limiting the total chat history to 1000 tokens, \r\n",
        "    chat_history_ids = model.generate(\r\n",
        "        bot_input_ids, max_length=200,\r\n",
        "        pad_token_id=tokenizer.eos_token_id,  \r\n",
        "        no_repeat_ngram_size=3,       \r\n",
        "        do_sample=True, \r\n",
        "        top_k=100, \r\n",
        "        top_p=0.7,\r\n",
        "        temperature = 0.8\r\n",
        "    )\r\n",
        "    \r\n",
        "    # pretty print last ouput tokens from bot\r\n",
        "    print(\"RickBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:664: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> User:Hello, RIck\n",
            "RickBot: Yeah, Morty,  I think you got it.\n",
            ">> User:I'm not Morty\n",
            "RickBot: I think you're looking for Rick.\n",
            ">> User:Who's Rick?\n",
            "RickBot: Rick, Morty.\n",
            ">> User:Are you Morty?\n",
            "RickBot: I can't believe that's a thing.\n",
            ">> User:It is\n",
            "RickBot: It's a miracle that he's still alive.\n"
          ]
        }
      ],
      "metadata": {
        "id": "nIeqMwZktv7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "615a75aa-9d42-4fe1-b23f-9979d64eb8ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**User**:How are you, Rick? <br>\n",
        "**RickBot**: I'm fine. <br>\n",
        "**User**:Where is Morty? <br>\n",
        "**RickBot**: He's in a hospital. <br>\n",
        "**User**:Why? <br>\n",
        "**RickBot**: Well, that depends on who breaks first -- me or the hospital. I mean, you know, the one that keeps giving me pills. <br>\n",
        "**User**:I don't understand <br>\n",
        "**RickBot**: Whoa! Whoa! <br>\n",
        "**User**:I see, you took your pills <br>\n",
        "**RickBot**: Hey, I-I'm not proud of this. <br>"
      ],
      "metadata": {
        "id": "7zq2wJFtss1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will give an example of a few more dialogues to show how Ricky is our Rick now."
      ],
      "metadata": {
        "id": "fvxnAOkTBTIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](https://media.giphy.com/media/eNpXWzGIMRjIo4lXT8/giphy.gif)\n",
        "\n",
        "Image from [Giphy](https://giphy.com/)\n",
        "\n",
        "**User**:Who are you? <br>\n",
        "**RickBot**: A musician, I'm not a musician, but I'm pretty close to you. <br>\n",
        "**User**:What kind of musician are you? <br>\n",
        "**RickBot**: Who cares? I'm a musician. <br>\n",
        "**User**:Ok, play a song for me <br>\n",
        "**RickBot**: Shut up and listen to me, Morty. Listen to me. Listen! I'm the leader of a bunch of different death worlds, and I'm just trying to assemble a band. I'm gonna make this one-piece, and you're gonna be part of it. <br>\n",
        "**User**:ok <br>\n",
        "**RickBot**: Whoa, whoa, hey, who's this? <br>\n",
        "**User**:This is Morty <br>\n",
        "**RickBot**: Geez!!! <br>"
      ],
      "metadata": {
        "id": "K1gX23iKkd0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](https://i.kym-cdn.com/news/posts/desktop/000/000/247/Screenshot_21.jpg)\n",
        "\n",
        "Image from [news.knowyourmeme.com](https://news.knowyourmeme.com/news/elon-musks-rick-and-morty-cameo-is-a-meme-reference)\n",
        "\n",
        "**User**:What do you think about Elon Musk? <br>\n",
        "**RickBot**: I think about him a lot. <br>\n",
        "**User**:But what exactly do you think? <br>\n",
        "**RickBot**: He's not a bad guy. He's just... not a very charismatic one. <br>\n",
        "**User**:I think he is charismatic <br>\n",
        "**RickBot**: He is. <br>\n",
        "**User**:What else you can say about him? <br>\n",
        "**RickBot**: You don't have to say anything. He is the leader of your class. <br>\n",
        "**User**:ok, thank you <br>\n",
        "**RickBot**: !!!"
      ],
      "metadata": {
        "id": "aDiz2J0LBqaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](https://media.giphy.com/media/YSTmV6MWEkgW0HblZQ/giphy.gif)\n",
        "\n",
        "Image from [Giphy](https://giphy.com/)\n",
        "\n",
        "**User**:hey, go get me a burger <br>\n",
        "RickBot: Whoa, whoa, hold on a second, let's not get crazy. This is gonna be good. <br>\n",
        "**User**:I'm hungry! c'mon, buy me a burger or better two <br>\n",
        "**RickBot**: Holy crap. <br>\n",
        "**User**:no crappy burger, a good one! <br>\n",
        "**RickBot**: What the hell? <br>\n",
        "**User**:don't be so bad, feed me <br>\n",
        "**RickBot**: I'll be back as fast as I can, Morty. <br>\n",
        "**User**:good boy, and don't forget ketchup and fries <br>\n",
        "**RickBot**: !!!!!! <br>\n",
        "**User**:what a hell? can you reply with words? <br>\n",
        "**RickBot**: Yes!! I will!! We will go out on a date. <br>"
      ],
      "metadata": {
        "id": "ezq6Qtl_3NtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "Woi683Eyvttc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](https://media.giphy.com/media/YQBWwGLJtFDQIATck5/giphy.gif)\n",
        "\n",
        "Image from [Giphy](https://giphy.com/)\n",
        "\n",
        "Congratulations! Our virtual Rick is alive (almost)! With the help of fine-tuning our model on a small dataset, we were able to create a virtual character with whom we can conduct interesting dialogs. \n",
        "\n",
        "Using the proposed approach you can create many interesting virtual characters based on an arbitrary dialogs dataset (just a csv file with replicas, one replica per line)."
      ],
      "metadata": {
        "id": "aZmkXDkv4BAl"
      }
    }
  ]
}